{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on synthetic data (BJ-RNN baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.train_synthetic import run_synthetic_experiments\n",
    "from utils.results import get_joint_coverages, get_interval_widths, plot_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINES = ['BJRNN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static noise profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 8.5794\n",
      "Epoch:  50 | train loss: 7.9333\n",
      "Epoch:  100 | train loss: 9.0237\n",
      "Epoch:  150 | train loss: 10.7468\n",
      "Epoch:  200 | train loss: 9.4355\n",
      "Epoch:  250 | train loss: 7.3767\n",
      "Epoch:  300 | train loss: 8.4498\n",
      "Epoch:  350 | train loss: 9.0975\n",
      "Epoch:  400 | train loss: 9.6869\n",
      "Epoch:  450 | train loss: 9.5128\n",
      "Epoch:  500 | train loss: 9.1163\n",
      "Epoch:  550 | train loss: 9.1743\n",
      "Epoch:  600 | train loss: 9.5061\n",
      "Epoch:  650 | train loss: 10.4087\n",
      "Epoch:  700 | train loss: 9.0968\n",
      "Epoch:  750 | train loss: 9.6103\n",
      "Epoch:  800 | train loss: 8.4667\n",
      "Epoch:  850 | train loss: 11.1679\n",
      "Epoch:  900 | train loss: 12.3933\n",
      "Epoch:  950 | train loss: 10.2442\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 9.5642\n",
      "Epoch:  50 | train loss: 9.3675\n",
      "Epoch:  100 | train loss: 9.0132\n",
      "Epoch:  150 | train loss: 8.5782\n",
      "Epoch:  200 | train loss: 8.5565\n",
      "Epoch:  250 | train loss: 9.4200\n",
      "Epoch:  300 | train loss: 11.1462\n",
      "Epoch:  350 | train loss: 10.1542\n",
      "Epoch:  400 | train loss: 10.5762\n",
      "Epoch:  450 | train loss: 9.2686\n",
      "Epoch:  500 | train loss: 8.6816\n",
      "Epoch:  550 | train loss: 10.1971\n",
      "Epoch:  600 | train loss: 8.7572\n",
      "Epoch:  650 | train loss: 9.5099\n",
      "Epoch:  700 | train loss: 9.1874\n",
      "Epoch:  750 | train loss: 8.4595\n",
      "Epoch:  800 | train loss: 10.2162\n",
      "Epoch:  850 | train loss: 8.8072\n",
      "Epoch:  900 | train loss: 9.8067\n",
      "Epoch:  950 | train loss: 9.5804\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 9.6431\n",
      "Epoch:  50 | train loss: 9.6654\n",
      "Epoch:  100 | train loss: 10.0450\n",
      "Epoch:  150 | train loss: 9.8718\n",
      "Epoch:  200 | train loss: 10.0163\n",
      "Epoch:  250 | train loss: 9.5036\n",
      "Epoch:  300 | train loss: 10.7684\n",
      "Epoch:  350 | train loss: 9.6114\n",
      "Epoch:  400 | train loss: 10.8111\n",
      "Epoch:  450 | train loss: 10.8355\n",
      "Epoch:  500 | train loss: 9.0378\n",
      "Epoch:  550 | train loss: 11.8558\n",
      "Epoch:  600 | train loss: 11.3975\n",
      "Epoch:  650 | train loss: 8.9989\n",
      "Epoch:  700 | train loss: 7.8393\n",
      "Epoch:  750 | train loss: 9.4642\n",
      "Epoch:  800 | train loss: 11.1859\n",
      "Epoch:  850 | train loss: 11.2853\n",
      "Epoch:  900 | train loss: 10.3704\n",
      "Epoch:  950 | train loss: 10.0075\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 8.8958\n",
      "Epoch:  50 | train loss: 8.0363\n",
      "Epoch:  100 | train loss: 9.0911\n",
      "Epoch:  150 | train loss: 8.8053\n",
      "Epoch:  200 | train loss: 9.6647\n",
      "Epoch:  250 | train loss: 9.7387\n",
      "Epoch:  300 | train loss: 10.3621\n",
      "Epoch:  350 | train loss: 8.1795\n",
      "Epoch:  400 | train loss: 11.5356\n",
      "Epoch:  450 | train loss: 9.9439\n",
      "Epoch:  500 | train loss: 9.6988\n",
      "Epoch:  550 | train loss: 9.8426\n",
      "Epoch:  600 | train loss: 7.3762\n",
      "Epoch:  650 | train loss: 8.5673\n",
      "Epoch:  700 | train loss: 9.5960\n",
      "Epoch:  750 | train loss: 8.5021\n",
      "Epoch:  800 | train loss: 10.1321\n",
      "Epoch:  850 | train loss: 8.9003\n",
      "Epoch:  900 | train loss: 8.7235\n",
      "Epoch:  950 | train loss: 8.5334\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 9.4449\n",
      "Epoch:  50 | train loss: 8.5485\n",
      "Epoch:  100 | train loss: 9.0153\n",
      "Epoch:  150 | train loss: 8.8052\n",
      "Epoch:  200 | train loss: 9.9556\n",
      "Epoch:  250 | train loss: 9.8723\n",
      "Epoch:  300 | train loss: 8.1864\n",
      "Epoch:  350 | train loss: 10.5160\n",
      "Epoch:  400 | train loss: 10.1366\n",
      "Epoch:  450 | train loss: 7.9260\n",
      "Epoch:  500 | train loss: 9.5714\n",
      "Epoch:  550 | train loss: 10.4706\n",
      "Epoch:  600 | train loss: 11.1398\n",
      "Epoch:  650 | train loss: 9.3236\n",
      "Epoch:  700 | train loss: 8.0155\n",
      "Epoch:  750 | train loss: 8.9448\n",
      "Epoch:  800 | train loss: 8.9503\n",
      "Epoch:  850 | train loss: 8.9921\n",
      "Epoch:  900 | train loss: 10.3979\n",
      "Epoch:  950 | train loss: 10.7839\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 9.7230\n",
      "Epoch:  50 | train loss: 9.1212\n",
      "Epoch:  100 | train loss: 9.0191\n",
      "Epoch:  150 | train loss: 10.3936\n",
      "Epoch:  200 | train loss: 10.8917\n",
      "Epoch:  250 | train loss: 8.9126\n",
      "Epoch:  300 | train loss: 11.6728\n",
      "Epoch:  350 | train loss: 7.3986\n",
      "Epoch:  400 | train loss: 10.8616\n",
      "Epoch:  450 | train loss: 9.3595\n",
      "Epoch:  500 | train loss: 10.1840\n",
      "Epoch:  550 | train loss: 8.0489\n",
      "Epoch:  600 | train loss: 9.5167\n",
      "Epoch:  650 | train loss: 10.7822\n",
      "Epoch:  700 | train loss: 10.3115\n",
      "Epoch:  750 | train loss: 8.4619\n",
      "Epoch:  800 | train loss: 7.7297\n",
      "Epoch:  850 | train loss: 11.0666\n",
      "Epoch:  900 | train loss: 8.2847\n",
      "Epoch:  950 | train loss: 8.7116\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 8.8928\n",
      "Epoch:  50 | train loss: 8.3487\n",
      "Epoch:  100 | train loss: 8.9378\n",
      "Epoch:  150 | train loss: 8.2630\n",
      "Epoch:  200 | train loss: 8.7620\n",
      "Epoch:  250 | train loss: 8.3753\n",
      "Epoch:  300 | train loss: 8.6542\n",
      "Epoch:  350 | train loss: 9.6797\n",
      "Epoch:  400 | train loss: 9.6304\n",
      "Epoch:  450 | train loss: 8.7581\n",
      "Epoch:  500 | train loss: 11.8458\n",
      "Epoch:  550 | train loss: 8.6303\n",
      "Epoch:  600 | train loss: 9.0442\n",
      "Epoch:  650 | train loss: 8.5870\n",
      "Epoch:  700 | train loss: 8.7724\n",
      "Epoch:  750 | train loss: 10.7396\n",
      "Epoch:  800 | train loss: 10.2329\n",
      "Epoch:  850 | train loss: 10.3546\n",
      "Epoch:  900 | train loss: 9.0069\n",
      "Epoch:  950 | train loss: 9.0087\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 7.9053\n",
      "Epoch:  50 | train loss: 10.2430\n",
      "Epoch:  100 | train loss: 8.4040\n",
      "Epoch:  150 | train loss: 8.4152\n",
      "Epoch:  200 | train loss: 9.2688\n",
      "Epoch:  250 | train loss: 9.1428\n",
      "Epoch:  300 | train loss: 9.7721\n",
      "Epoch:  350 | train loss: 9.7509\n",
      "Epoch:  400 | train loss: 8.8064\n",
      "Epoch:  450 | train loss: 8.8356\n",
      "Epoch:  500 | train loss: 9.5420\n",
      "Epoch:  550 | train loss: 11.2638\n",
      "Epoch:  600 | train loss: 11.8298\n",
      "Epoch:  650 | train loss: 9.9602\n",
      "Epoch:  700 | train loss: 10.1977\n",
      "Epoch:  750 | train loss: 8.5602\n",
      "Epoch:  800 | train loss: 10.4182\n",
      "Epoch:  850 | train loss: 8.3022\n",
      "Epoch:  900 | train loss: 8.6547\n",
      "Epoch:  950 | train loss: 9.6910\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 9.8226\n",
      "Epoch:  50 | train loss: 9.6018\n",
      "Epoch:  100 | train loss: 8.0564\n",
      "Epoch:  150 | train loss: 9.6092\n",
      "Epoch:  200 | train loss: 9.5942\n",
      "Epoch:  250 | train loss: 11.7265\n",
      "Epoch:  300 | train loss: 7.3184\n",
      "Epoch:  350 | train loss: 10.4158\n",
      "Epoch:  400 | train loss: 9.6657\n",
      "Epoch:  450 | train loss: 10.6092\n",
      "Epoch:  500 | train loss: 9.5808\n",
      "Epoch:  550 | train loss: 8.4028\n",
      "Epoch:  600 | train loss: 7.7533\n",
      "Epoch:  650 | train loss: 9.1920\n",
      "Epoch:  700 | train loss: 9.1156\n",
      "Epoch:  750 | train loss: 9.5610\n",
      "Epoch:  800 | train loss: 9.5698\n",
      "Epoch:  850 | train loss: 10.3425\n",
      "Epoch:  900 | train loss: 9.3208\n",
      "Epoch:  950 | train loss: 9.2987\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 13.5412\n",
      "Epoch:  50 | train loss: 9.0384\n",
      "Epoch:  100 | train loss: 9.3819\n",
      "Epoch:  150 | train loss: 8.7550\n",
      "Epoch:  200 | train loss: 8.9775\n",
      "Epoch:  250 | train loss: 11.7352\n",
      "Epoch:  300 | train loss: 11.2587\n",
      "Epoch:  350 | train loss: 9.1593\n",
      "Epoch:  400 | train loss: 12.9354\n",
      "Epoch:  450 | train loss: 9.0658\n",
      "Epoch:  500 | train loss: 10.2319\n",
      "Epoch:  550 | train loss: 10.2228\n",
      "Epoch:  600 | train loss: 12.2141\n",
      "Epoch:  650 | train loss: 8.9040\n",
      "Epoch:  700 | train loss: 9.5099\n",
      "Epoch:  750 | train loss: 9.7250\n",
      "Epoch:  800 | train loss: 9.6246\n",
      "Epoch:  850 | train loss: 8.7417\n",
      "Epoch:  900 | train loss: 10.4869\n",
      "Epoch:  950 | train loss: 9.3412\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 10.3557\n",
      "Epoch:  50 | train loss: 9.3481\n",
      "Epoch:  100 | train loss: 8.7480\n",
      "Epoch:  150 | train loss: 9.6224\n",
      "Epoch:  200 | train loss: 9.0418\n",
      "Epoch:  250 | train loss: 10.3046\n",
      "Epoch:  300 | train loss: 10.4605\n",
      "Epoch:  350 | train loss: 8.2277\n",
      "Epoch:  400 | train loss: 8.7397\n",
      "Epoch:  450 | train loss: 11.3064\n",
      "Epoch:  500 | train loss: 14.2221\n",
      "Epoch:  550 | train loss: 11.1118\n",
      "Epoch:  600 | train loss: 10.4461\n",
      "Epoch:  650 | train loss: 11.1934\n",
      "Epoch:  700 | train loss: 10.8535\n",
      "Epoch:  750 | train loss: 11.5569\n",
      "Epoch:  800 | train loss: 9.4300\n",
      "Epoch:  850 | train loss: 9.3980\n",
      "Epoch:  900 | train loss: 10.1265\n",
      "Epoch:  950 | train loss: 10.2955\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 9.3844\n",
      "Epoch:  50 | train loss: 8.0997\n",
      "Epoch:  100 | train loss: 8.9095\n",
      "Epoch:  150 | train loss: 8.2076\n",
      "Epoch:  200 | train loss: 9.4574\n",
      "Epoch:  250 | train loss: 8.8235\n",
      "Epoch:  300 | train loss: 8.8763\n",
      "Epoch:  350 | train loss: 8.6952\n",
      "Epoch:  400 | train loss: 7.5442\n",
      "Epoch:  450 | train loss: 8.3595\n",
      "Epoch:  500 | train loss: 6.8213\n",
      "Epoch:  550 | train loss: 11.2334\n",
      "Epoch:  600 | train loss: 9.5576\n",
      "Epoch:  650 | train loss: 7.9525\n",
      "Epoch:  700 | train loss: 8.9943\n",
      "Epoch:  750 | train loss: 8.5604\n",
      "Epoch:  800 | train loss: 8.3959\n",
      "Epoch:  850 | train loss: 7.7635\n",
      "Epoch:  900 | train loss: 9.3791\n",
      "Epoch:  950 | train loss: 7.7549\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 8.7233\n",
      "Epoch:  50 | train loss: 8.1647\n",
      "Epoch:  100 | train loss: 9.7815\n",
      "Epoch:  150 | train loss: 8.6980\n",
      "Epoch:  200 | train loss: 7.5853\n",
      "Epoch:  250 | train loss: 10.9424\n",
      "Epoch:  300 | train loss: 8.2503\n",
      "Epoch:  350 | train loss: 9.5991\n",
      "Epoch:  400 | train loss: 9.0781\n",
      "Epoch:  450 | train loss: 8.0851\n",
      "Epoch:  500 | train loss: 9.1669\n",
      "Epoch:  550 | train loss: 10.1096\n",
      "Epoch:  600 | train loss: 9.6572\n",
      "Epoch:  650 | train loss: 9.7012\n",
      "Epoch:  700 | train loss: 9.5876\n",
      "Epoch:  750 | train loss: 8.9806\n",
      "Epoch:  800 | train loss: 8.7738\n",
      "Epoch:  850 | train loss: 9.2306\n",
      "Epoch:  900 | train loss: 9.8106\n",
      "Epoch:  950 | train loss: 8.0446\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 9.3054\n",
      "Epoch:  50 | train loss: 12.2700\n",
      "Epoch:  100 | train loss: 9.7501\n",
      "Epoch:  150 | train loss: 11.0358\n",
      "Epoch:  200 | train loss: 8.0667\n",
      "Epoch:  250 | train loss: 8.5707\n",
      "Epoch:  300 | train loss: 11.6396\n",
      "Epoch:  350 | train loss: 10.2322\n",
      "Epoch:  400 | train loss: 8.3246\n",
      "Epoch:  450 | train loss: 8.6531\n",
      "Epoch:  500 | train loss: 11.0353\n",
      "Epoch:  550 | train loss: 10.2405\n",
      "Epoch:  600 | train loss: 9.1308\n",
      "Epoch:  650 | train loss: 10.4481\n",
      "Epoch:  700 | train loss: 8.6978\n",
      "Epoch:  750 | train loss: 8.3486\n",
      "Epoch:  800 | train loss: 11.4906\n",
      "Epoch:  850 | train loss: 8.9880\n",
      "Epoch:  900 | train loss: 9.8498\n",
      "Epoch:  950 | train loss: 8.7907\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 10.0478\n",
      "Epoch:  50 | train loss: 11.7099\n",
      "Epoch:  100 | train loss: 9.8393\n",
      "Epoch:  150 | train loss: 10.9655\n",
      "Epoch:  200 | train loss: 9.2920\n",
      "Epoch:  250 | train loss: 10.0953\n",
      "Epoch:  300 | train loss: 11.3152\n",
      "Epoch:  350 | train loss: 8.3434\n",
      "Epoch:  400 | train loss: 7.8585\n",
      "Epoch:  450 | train loss: 8.2217\n",
      "Epoch:  500 | train loss: 9.4441\n",
      "Epoch:  550 | train loss: 10.8017\n",
      "Epoch:  600 | train loss: 8.0798\n",
      "Epoch:  650 | train loss: 8.1497\n",
      "Epoch:  700 | train loss: 9.9127\n",
      "Epoch:  750 | train loss: 8.7902\n",
      "Epoch:  800 | train loss: 8.9475\n",
      "Epoch:  850 | train loss: 9.0402\n",
      "Epoch:  900 | train loss: 8.6763\n",
      "Epoch:  950 | train loss: 10.5389\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 9.9731\n",
      "Epoch:  50 | train loss: 7.5279\n",
      "Epoch:  100 | train loss: 8.0769\n",
      "Epoch:  150 | train loss: 9.3190\n",
      "Epoch:  200 | train loss: 9.7114\n",
      "Epoch:  250 | train loss: 11.6628\n",
      "Epoch:  300 | train loss: 7.8494\n",
      "Epoch:  350 | train loss: 7.5511\n",
      "Epoch:  400 | train loss: 9.4846\n",
      "Epoch:  450 | train loss: 8.9319\n",
      "Epoch:  500 | train loss: 7.9952\n",
      "Epoch:  550 | train loss: 7.8292\n",
      "Epoch:  600 | train loss: 8.4176\n",
      "Epoch:  650 | train loss: 9.4231\n",
      "Epoch:  700 | train loss: 9.4633\n",
      "Epoch:  750 | train loss: 8.2519\n",
      "Epoch:  800 | train loss: 8.7838\n",
      "Epoch:  850 | train loss: 9.2515\n",
      "Epoch:  900 | train loss: 9.1788\n",
      "Epoch:  950 | train loss: 7.9299\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 8.4689\n",
      "Epoch:  50 | train loss: 7.7785\n",
      "Epoch:  100 | train loss: 8.5133\n",
      "Epoch:  150 | train loss: 9.4622\n",
      "Epoch:  200 | train loss: 7.7876\n",
      "Epoch:  250 | train loss: 8.9950\n",
      "Epoch:  300 | train loss: 9.6548\n",
      "Epoch:  350 | train loss: 7.0938\n",
      "Epoch:  400 | train loss: 8.9692\n",
      "Epoch:  450 | train loss: 9.5449\n",
      "Epoch:  500 | train loss: 12.5841\n",
      "Epoch:  550 | train loss: 9.5404\n",
      "Epoch:  600 | train loss: 9.4165\n",
      "Epoch:  650 | train loss: 9.5194\n",
      "Epoch:  700 | train loss: 8.2366\n",
      "Epoch:  750 | train loss: 8.7011\n",
      "Epoch:  800 | train loss: 8.6716\n",
      "Epoch:  850 | train loss: 10.4229\n",
      "Epoch:  900 | train loss: 8.7910\n",
      "Epoch:  950 | train loss: 8.5011\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 11.0768\n",
      "Epoch:  50 | train loss: 11.2504\n",
      "Epoch:  100 | train loss: 8.1118\n",
      "Epoch:  150 | train loss: 9.6958\n",
      "Epoch:  200 | train loss: 9.2270\n",
      "Epoch:  250 | train loss: 9.6005\n",
      "Epoch:  300 | train loss: 9.7161\n",
      "Epoch:  350 | train loss: 8.5183\n",
      "Epoch:  400 | train loss: 9.8538\n",
      "Epoch:  450 | train loss: 11.0291\n",
      "Epoch:  500 | train loss: 8.5821\n",
      "Epoch:  550 | train loss: 9.7948\n",
      "Epoch:  600 | train loss: 9.9844\n",
      "Epoch:  650 | train loss: 10.2528\n",
      "Epoch:  700 | train loss: 9.5192\n",
      "Epoch:  750 | train loss: 9.5270\n",
      "Epoch:  800 | train loss: 8.5604\n",
      "Epoch:  850 | train loss: 8.1912\n",
      "Epoch:  900 | train loss: 9.5466\n",
      "Epoch:  950 | train loss: 12.8446\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 9.7287\n",
      "Epoch:  50 | train loss: 10.1262\n",
      "Epoch:  100 | train loss: 8.2253\n",
      "Epoch:  150 | train loss: 9.7884\n",
      "Epoch:  200 | train loss: 10.1715\n",
      "Epoch:  250 | train loss: 11.1552\n",
      "Epoch:  300 | train loss: 8.8364\n",
      "Epoch:  350 | train loss: 10.5606\n",
      "Epoch:  400 | train loss: 11.5152\n",
      "Epoch:  450 | train loss: 10.9785\n",
      "Epoch:  500 | train loss: 10.4592\n",
      "Epoch:  550 | train loss: 11.5555\n",
      "Epoch:  600 | train loss: 11.5838\n",
      "Epoch:  650 | train loss: 10.3006\n",
      "Epoch:  700 | train loss: 11.3530\n",
      "Epoch:  750 | train loss: 8.8165\n",
      "Epoch:  800 | train loss: 9.6324\n",
      "Epoch:  850 | train loss: 11.8607\n",
      "Epoch:  900 | train loss: 9.7984\n",
      "Epoch:  950 | train loss: 10.2352\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 9.5366\n",
      "Epoch:  50 | train loss: 9.4356\n",
      "Epoch:  100 | train loss: 10.3469\n",
      "Epoch:  150 | train loss: 10.4824\n",
      "Epoch:  200 | train loss: 10.4040\n",
      "Epoch:  250 | train loss: 9.4188\n",
      "Epoch:  300 | train loss: 10.2650\n",
      "Epoch:  350 | train loss: 9.3755\n",
      "Epoch:  400 | train loss: 7.3348\n",
      "Epoch:  450 | train loss: 8.7911\n",
      "Epoch:  500 | train loss: 10.0021\n",
      "Epoch:  550 | train loss: 11.1124\n",
      "Epoch:  600 | train loss: 8.8890\n",
      "Epoch:  650 | train loss: 10.4922\n",
      "Epoch:  700 | train loss: 8.6602\n",
      "Epoch:  750 | train loss: 10.1316\n",
      "Epoch:  800 | train loss: 12.1937\n",
      "Epoch:  850 | train loss: 11.6753\n",
      "Epoch:  900 | train loss: 9.4084\n",
      "Epoch:  950 | train loss: 9.5982\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 8.1256\n",
      "Epoch:  50 | train loss: 10.4865\n",
      "Epoch:  100 | train loss: 7.7474\n",
      "Epoch:  150 | train loss: 8.4016\n",
      "Epoch:  200 | train loss: 8.4421\n",
      "Epoch:  250 | train loss: 9.9957\n",
      "Epoch:  300 | train loss: 8.0667\n",
      "Epoch:  350 | train loss: 10.0447\n",
      "Epoch:  400 | train loss: 8.8902\n",
      "Epoch:  450 | train loss: 9.7835\n",
      "Epoch:  500 | train loss: 11.0949\n",
      "Epoch:  550 | train loss: 8.3861\n",
      "Epoch:  600 | train loss: 9.8450\n",
      "Epoch:  650 | train loss: 8.7875\n",
      "Epoch:  700 | train loss: 8.1099\n",
      "Epoch:  750 | train loss: 9.3539\n",
      "Epoch:  800 | train loss: 9.6807\n",
      "Epoch:  850 | train loss: 9.7251\n",
      "Epoch:  900 | train loss: 9.1582\n",
      "Epoch:  950 | train loss: 9.2187\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 8.9905\n",
      "Epoch:  50 | train loss: 9.4668\n",
      "Epoch:  100 | train loss: 10.6630\n",
      "Epoch:  150 | train loss: 9.6189\n",
      "Epoch:  200 | train loss: 9.5242\n",
      "Epoch:  250 | train loss: 8.9635\n",
      "Epoch:  300 | train loss: 13.0228\n",
      "Epoch:  350 | train loss: 10.6856\n",
      "Epoch:  400 | train loss: 9.9915\n",
      "Epoch:  450 | train loss: 10.0809\n",
      "Epoch:  500 | train loss: 10.2911\n",
      "Epoch:  550 | train loss: 11.5470\n",
      "Epoch:  600 | train loss: 9.3848\n",
      "Epoch:  650 | train loss: 11.6729\n",
      "Epoch:  700 | train loss: 10.1478\n",
      "Epoch:  750 | train loss: 11.6863\n",
      "Epoch:  800 | train loss: 11.8415\n",
      "Epoch:  850 | train loss: 8.9675\n",
      "Epoch:  900 | train loss: 8.6916\n",
      "Epoch:  950 | train loss: 9.4403\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 6.9179\n",
      "Epoch:  50 | train loss: 8.9884\n",
      "Epoch:  100 | train loss: 8.0248\n",
      "Epoch:  150 | train loss: 10.0494\n",
      "Epoch:  200 | train loss: 9.8860\n",
      "Epoch:  250 | train loss: 10.8705\n",
      "Epoch:  300 | train loss: 10.4651\n",
      "Epoch:  350 | train loss: 9.8125\n",
      "Epoch:  400 | train loss: 10.7057\n",
      "Epoch:  450 | train loss: 10.3257\n",
      "Epoch:  500 | train loss: 8.3497\n",
      "Epoch:  550 | train loss: 10.6568\n",
      "Epoch:  600 | train loss: 9.0745\n",
      "Epoch:  650 | train loss: 9.0742\n",
      "Epoch:  700 | train loss: 9.3275\n",
      "Epoch:  750 | train loss: 8.7466\n",
      "Epoch:  800 | train loss: 8.9817\n",
      "Epoch:  850 | train loss: 10.3145\n",
      "Epoch:  900 | train loss: 9.5001\n",
      "Epoch:  950 | train loss: 8.6963\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 9.9881\n",
      "Epoch:  50 | train loss: 9.9099\n",
      "Epoch:  100 | train loss: 9.4032\n",
      "Epoch:  150 | train loss: 9.7137\n",
      "Epoch:  200 | train loss: 10.4684\n",
      "Epoch:  250 | train loss: 7.9585\n",
      "Epoch:  300 | train loss: 9.1067\n",
      "Epoch:  350 | train loss: 11.1157\n",
      "Epoch:  400 | train loss: 11.4473\n",
      "Epoch:  450 | train loss: 8.5273\n",
      "Epoch:  500 | train loss: 9.4028\n",
      "Epoch:  550 | train loss: 9.7436\n",
      "Epoch:  600 | train loss: 10.1935\n",
      "Epoch:  650 | train loss: 11.4024\n",
      "Epoch:  700 | train loss: 11.3070\n",
      "Epoch:  750 | train loss: 9.9134\n",
      "Epoch:  800 | train loss: 9.7445\n",
      "Epoch:  850 | train loss: 9.8709\n",
      "Epoch:  900 | train loss: 9.9560\n",
      "Epoch:  950 | train loss: 10.8977\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 11.7384\n",
      "Epoch:  50 | train loss: 7.4095\n",
      "Epoch:  100 | train loss: 9.6861\n",
      "Epoch:  150 | train loss: 11.3642\n",
      "Epoch:  200 | train loss: 9.1113\n",
      "Epoch:  250 | train loss: 7.4364\n",
      "Epoch:  300 | train loss: 10.5026\n",
      "Epoch:  350 | train loss: 9.3077\n",
      "Epoch:  400 | train loss: 8.3370\n",
      "Epoch:  450 | train loss: 8.6235\n",
      "Epoch:  500 | train loss: 9.9259\n",
      "Epoch:  550 | train loss: 8.2445\n",
      "Epoch:  600 | train loss: 8.7255\n",
      "Epoch:  650 | train loss: 8.5458\n",
      "Epoch:  700 | train loss: 9.4651\n",
      "Epoch:  750 | train loss: 9.7176\n",
      "Epoch:  800 | train loss: 8.9180\n",
      "Epoch:  850 | train loss: 7.2582\n",
      "Epoch:  900 | train loss: 8.5307\n",
      "Epoch:  950 | train loss: 8.8375\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    for seed in range(5):\n",
    "        run_synthetic_experiments(experiment='static', \n",
    "                                  baseline=baseline,\n",
    "                                  n_train = 2000,\n",
    "                                  retrain_auxiliary=True,\n",
    "                                  recompute_dataset=True,\n",
    "                                  save_model=True, \n",
    "                                  save_results=True,\n",
    "                                  rnn_mode='RNN',\n",
    "                                  seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint coverage** \n",
    "\n",
    "Prints mean joint coverage across the horizon (meanÂ±std of 5 random seeds, with each row indicating a different dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJRNN\n",
      "99.9 \\(\\pm\\) 0.2\\%\n",
      "99.8 \\(\\pm\\) 0.1\\%\n",
      "99.6 \\(\\pm\\) 0.4\\%\n",
      "100.0 \\(\\pm\\) 0.1\\%\n",
      "99.9 \\(\\pm\\) 0.2\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_coverages(baseline, 'static', seeds=range(5))\n",
    "    \n",
    "    for m, s in zip(coverages_mean, coverages_std):\n",
    "        print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(m, s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJRNN\n",
      "[30.69915917 26.15965366 26.13531172 26.40026501 27.57770923]\n",
      "[11.66782423  2.34472132  2.24957157  1.50863427  2.22065518]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_interval_widths(baseline, 'static', seeds=range(5))\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-dependent noise profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training procedure for CF-RNN/MQ-RNN/DP-RNN baselines (for 5 different seeds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 13.9886\n",
      "Epoch:  50 | train loss: 12.6675\n",
      "Epoch:  100 | train loss: 11.1967\n",
      "Epoch:  150 | train loss: 13.9567\n",
      "Epoch:  200 | train loss: 14.0799\n",
      "Epoch:  250 | train loss: 15.6307\n",
      "Epoch:  300 | train loss: 15.0454\n",
      "Epoch:  350 | train loss: 14.9318\n",
      "Epoch:  400 | train loss: 12.5476\n",
      "Epoch:  450 | train loss: 11.7732\n",
      "Epoch:  500 | train loss: 12.5791\n",
      "Epoch:  550 | train loss: 12.7417\n",
      "Epoch:  600 | train loss: 14.9488\n",
      "Epoch:  650 | train loss: 10.7527\n",
      "Epoch:  700 | train loss: 13.7926\n",
      "Epoch:  750 | train loss: 10.8014\n",
      "Epoch:  800 | train loss: 10.9499\n",
      "Epoch:  850 | train loss: 14.3753\n",
      "Epoch:  900 | train loss: 12.3114\n",
      "Epoch:  950 | train loss: 13.3183\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 25.6105\n",
      "Epoch:  50 | train loss: 24.3140\n",
      "Epoch:  100 | train loss: 20.3995\n",
      "Epoch:  150 | train loss: 24.3123\n",
      "Epoch:  200 | train loss: 23.1713\n",
      "Epoch:  250 | train loss: 25.0532\n",
      "Epoch:  300 | train loss: 23.0256\n",
      "Epoch:  350 | train loss: 21.3599\n",
      "Epoch:  400 | train loss: 20.6231\n",
      "Epoch:  450 | train loss: 21.5625\n",
      "Epoch:  500 | train loss: 24.0190\n",
      "Epoch:  550 | train loss: 22.4649\n",
      "Epoch:  600 | train loss: 21.5240\n",
      "Epoch:  650 | train loss: 20.4723\n",
      "Epoch:  700 | train loss: 25.4079\n",
      "Epoch:  750 | train loss: 25.9031\n",
      "Epoch:  800 | train loss: 23.1651\n",
      "Epoch:  850 | train loss: 24.2831\n",
      "Epoch:  900 | train loss: 24.7983\n",
      "Epoch:  950 | train loss: 26.6841\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 40.1725\n",
      "Epoch:  50 | train loss: 36.8044\n",
      "Epoch:  100 | train loss: 41.1147\n",
      "Epoch:  150 | train loss: 39.0341\n",
      "Epoch:  200 | train loss: 43.1016\n",
      "Epoch:  250 | train loss: 32.8240\n",
      "Epoch:  300 | train loss: 40.2922\n",
      "Epoch:  350 | train loss: 38.6379\n",
      "Epoch:  400 | train loss: 39.9220\n",
      "Epoch:  450 | train loss: 43.4558\n",
      "Epoch:  500 | train loss: 39.5634\n",
      "Epoch:  550 | train loss: 35.9906\n",
      "Epoch:  600 | train loss: 38.1265\n",
      "Epoch:  650 | train loss: 38.5136\n",
      "Epoch:  700 | train loss: 39.3076\n",
      "Epoch:  750 | train loss: 35.7262\n",
      "Epoch:  800 | train loss: 35.3763\n",
      "Epoch:  850 | train loss: 41.4896\n",
      "Epoch:  900 | train loss: 37.7195\n",
      "Epoch:  950 | train loss: 34.4429\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 57.9949\n",
      "Epoch:  50 | train loss: 59.8798\n",
      "Epoch:  100 | train loss: 60.2087\n",
      "Epoch:  150 | train loss: 56.9394\n",
      "Epoch:  200 | train loss: 49.5907\n",
      "Epoch:  250 | train loss: 61.0812\n",
      "Epoch:  300 | train loss: 57.2965\n",
      "Epoch:  350 | train loss: 64.6456\n",
      "Epoch:  400 | train loss: 49.0276\n",
      "Epoch:  450 | train loss: 55.2463\n",
      "Epoch:  500 | train loss: 56.3758\n",
      "Epoch:  550 | train loss: 65.7986\n",
      "Epoch:  600 | train loss: 55.5085\n",
      "Epoch:  650 | train loss: 56.4968\n",
      "Epoch:  700 | train loss: 63.5649\n",
      "Epoch:  750 | train loss: 49.8329\n",
      "Epoch:  800 | train loss: 63.1091\n",
      "Epoch:  850 | train loss: 62.4304\n",
      "Epoch:  900 | train loss: 52.7818\n",
      "Epoch:  950 | train loss: 50.5081\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 90.4451\n",
      "Epoch:  50 | train loss: 91.5076\n",
      "Epoch:  100 | train loss: 92.3707\n",
      "Epoch:  150 | train loss: 90.0712\n",
      "Epoch:  200 | train loss: 86.5087\n",
      "Epoch:  250 | train loss: 88.2613\n",
      "Epoch:  300 | train loss: 88.1116\n",
      "Epoch:  350 | train loss: 93.2097\n",
      "Epoch:  400 | train loss: 97.5072\n",
      "Epoch:  450 | train loss: 85.8322\n",
      "Epoch:  500 | train loss: 85.6288\n",
      "Epoch:  550 | train loss: 92.1560\n",
      "Epoch:  600 | train loss: 88.2833\n",
      "Epoch:  650 | train loss: 80.9600\n",
      "Epoch:  700 | train loss: 85.2923\n",
      "Epoch:  750 | train loss: 95.4215\n",
      "Epoch:  800 | train loss: 82.8413\n",
      "Epoch:  850 | train loss: 79.1107\n",
      "Epoch:  900 | train loss: 83.2543\n",
      "Epoch:  950 | train loss: 78.1837\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 11.6798\n",
      "Epoch:  50 | train loss: 11.7668\n",
      "Epoch:  100 | train loss: 13.2117\n",
      "Epoch:  150 | train loss: 14.3611\n",
      "Epoch:  200 | train loss: 11.7536\n",
      "Epoch:  250 | train loss: 13.1784\n",
      "Epoch:  300 | train loss: 11.9919\n",
      "Epoch:  350 | train loss: 14.7510\n",
      "Epoch:  400 | train loss: 12.2319\n",
      "Epoch:  450 | train loss: 12.4095\n",
      "Epoch:  500 | train loss: 12.7967\n",
      "Epoch:  550 | train loss: 16.1290\n",
      "Epoch:  600 | train loss: 12.3435\n",
      "Epoch:  650 | train loss: 13.1504\n",
      "Epoch:  700 | train loss: 13.6246\n",
      "Epoch:  750 | train loss: 13.1151\n",
      "Epoch:  800 | train loss: 13.5422\n",
      "Epoch:  850 | train loss: 15.7973\n",
      "Epoch:  900 | train loss: 11.8330\n",
      "Epoch:  950 | train loss: 12.7119\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 24.4530\n",
      "Epoch:  50 | train loss: 23.5277\n",
      "Epoch:  100 | train loss: 24.4310\n",
      "Epoch:  150 | train loss: 22.0177\n",
      "Epoch:  200 | train loss: 23.1907\n",
      "Epoch:  250 | train loss: 22.2389\n",
      "Epoch:  300 | train loss: 19.1543\n",
      "Epoch:  350 | train loss: 25.7468\n",
      "Epoch:  400 | train loss: 22.0507\n",
      "Epoch:  450 | train loss: 21.4002\n",
      "Epoch:  500 | train loss: 22.9530\n",
      "Epoch:  550 | train loss: 25.9590\n",
      "Epoch:  600 | train loss: 24.9281\n",
      "Epoch:  650 | train loss: 21.3115\n",
      "Epoch:  700 | train loss: 24.6755\n",
      "Epoch:  750 | train loss: 22.4398\n",
      "Epoch:  800 | train loss: 24.6973\n",
      "Epoch:  850 | train loss: 20.9442\n",
      "Epoch:  900 | train loss: 23.3811\n",
      "Epoch:  950 | train loss: 24.0242\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 35.6767\n",
      "Epoch:  50 | train loss: 36.9763\n",
      "Epoch:  100 | train loss: 35.1155\n",
      "Epoch:  150 | train loss: 39.2991\n",
      "Epoch:  200 | train loss: 39.5651\n",
      "Epoch:  250 | train loss: 39.5478\n",
      "Epoch:  300 | train loss: 36.7162\n",
      "Epoch:  350 | train loss: 34.8611\n",
      "Epoch:  400 | train loss: 39.5156\n",
      "Epoch:  450 | train loss: 35.1476\n",
      "Epoch:  500 | train loss: 35.2546\n",
      "Epoch:  550 | train loss: 35.8041\n",
      "Epoch:  600 | train loss: 39.1331\n",
      "Epoch:  650 | train loss: 34.5125\n",
      "Epoch:  700 | train loss: 39.5514\n",
      "Epoch:  750 | train loss: 38.7862\n",
      "Epoch:  800 | train loss: 36.7090\n",
      "Epoch:  850 | train loss: 39.2985\n",
      "Epoch:  900 | train loss: 39.3001\n",
      "Epoch:  950 | train loss: 43.2879\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 58.1827\n",
      "Epoch:  50 | train loss: 67.3481\n",
      "Epoch:  100 | train loss: 57.1513\n",
      "Epoch:  150 | train loss: 57.3899\n",
      "Epoch:  200 | train loss: 64.5492\n",
      "Epoch:  250 | train loss: 60.2849\n",
      "Epoch:  300 | train loss: 60.0825\n",
      "Epoch:  350 | train loss: 58.2824\n",
      "Epoch:  400 | train loss: 57.4455\n",
      "Epoch:  450 | train loss: 67.2651\n",
      "Epoch:  500 | train loss: 59.5941\n",
      "Epoch:  550 | train loss: 58.9687\n",
      "Epoch:  600 | train loss: 61.2308\n",
      "Epoch:  650 | train loss: 64.7446\n",
      "Epoch:  700 | train loss: 67.3019\n",
      "Epoch:  750 | train loss: 65.6365\n",
      "Epoch:  800 | train loss: 66.7363\n",
      "Epoch:  850 | train loss: 64.1712\n",
      "Epoch:  900 | train loss: 51.2796\n",
      "Epoch:  950 | train loss: 58.3678\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 89.4005\n",
      "Epoch:  50 | train loss: 88.0473\n",
      "Epoch:  100 | train loss: 83.8974\n",
      "Epoch:  150 | train loss: 88.5028\n",
      "Epoch:  200 | train loss: 85.9457\n",
      "Epoch:  250 | train loss: 87.8567\n",
      "Epoch:  300 | train loss: 80.6001\n",
      "Epoch:  350 | train loss: 88.7165\n",
      "Epoch:  400 | train loss: 83.8693\n",
      "Epoch:  450 | train loss: 87.9661\n",
      "Epoch:  500 | train loss: 88.3221\n",
      "Epoch:  550 | train loss: 84.1428\n",
      "Epoch:  600 | train loss: 85.3736\n",
      "Epoch:  650 | train loss: 94.8668\n",
      "Epoch:  700 | train loss: 89.1358\n",
      "Epoch:  750 | train loss: 89.5103\n",
      "Epoch:  800 | train loss: 81.3672\n",
      "Epoch:  850 | train loss: 88.6906\n",
      "Epoch:  900 | train loss: 93.5404\n",
      "Epoch:  950 | train loss: 86.2499\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 11.7847\n",
      "Epoch:  50 | train loss: 12.1542\n",
      "Epoch:  100 | train loss: 11.2110\n",
      "Epoch:  150 | train loss: 12.2257\n",
      "Epoch:  200 | train loss: 12.9317\n",
      "Epoch:  250 | train loss: 13.9016\n",
      "Epoch:  300 | train loss: 13.3730\n",
      "Epoch:  350 | train loss: 15.1610\n",
      "Epoch:  400 | train loss: 14.4638\n",
      "Epoch:  450 | train loss: 12.7100\n",
      "Epoch:  500 | train loss: 13.8658\n",
      "Epoch:  550 | train loss: 14.7590\n",
      "Epoch:  600 | train loss: 15.1921\n",
      "Epoch:  650 | train loss: 12.2817\n",
      "Epoch:  700 | train loss: 12.2786\n",
      "Epoch:  750 | train loss: 14.8950\n",
      "Epoch:  800 | train loss: 13.6413\n",
      "Epoch:  850 | train loss: 13.3299\n",
      "Epoch:  900 | train loss: 11.4917\n",
      "Epoch:  950 | train loss: 13.9157\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 23.4994\n",
      "Epoch:  50 | train loss: 18.0404\n",
      "Epoch:  100 | train loss: 22.8850\n",
      "Epoch:  150 | train loss: 23.8088\n",
      "Epoch:  200 | train loss: 21.1796\n",
      "Epoch:  250 | train loss: 26.2116\n",
      "Epoch:  300 | train loss: 22.2932\n",
      "Epoch:  350 | train loss: 22.5872\n",
      "Epoch:  400 | train loss: 19.7920\n",
      "Epoch:  450 | train loss: 22.4070\n",
      "Epoch:  500 | train loss: 22.0942\n",
      "Epoch:  550 | train loss: 23.3161\n",
      "Epoch:  600 | train loss: 22.7185\n",
      "Epoch:  650 | train loss: 22.0943\n",
      "Epoch:  700 | train loss: 25.4562\n",
      "Epoch:  750 | train loss: 20.8667\n",
      "Epoch:  800 | train loss: 18.9540\n",
      "Epoch:  850 | train loss: 21.9986\n",
      "Epoch:  900 | train loss: 21.7114\n",
      "Epoch:  950 | train loss: 19.0522\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 36.4081\n",
      "Epoch:  50 | train loss: 39.8673\n",
      "Epoch:  100 | train loss: 39.1613\n",
      "Epoch:  150 | train loss: 38.7336\n",
      "Epoch:  200 | train loss: 36.3091\n",
      "Epoch:  250 | train loss: 38.6483\n",
      "Epoch:  300 | train loss: 39.0228\n",
      "Epoch:  350 | train loss: 41.2494\n",
      "Epoch:  400 | train loss: 40.4034\n",
      "Epoch:  450 | train loss: 41.0231\n",
      "Epoch:  500 | train loss: 38.7738\n",
      "Epoch:  550 | train loss: 39.6308\n",
      "Epoch:  600 | train loss: 37.1228\n",
      "Epoch:  650 | train loss: 42.8308\n",
      "Epoch:  700 | train loss: 37.8596\n",
      "Epoch:  750 | train loss: 36.6537\n",
      "Epoch:  800 | train loss: 38.7648\n",
      "Epoch:  850 | train loss: 38.2351\n",
      "Epoch:  900 | train loss: 39.6056\n",
      "Epoch:  950 | train loss: 39.2105\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 55.8120\n",
      "Epoch:  50 | train loss: 60.9461\n",
      "Epoch:  100 | train loss: 65.6577\n",
      "Epoch:  150 | train loss: 73.6814\n",
      "Epoch:  200 | train loss: 54.8306\n",
      "Epoch:  250 | train loss: 60.3723\n",
      "Epoch:  300 | train loss: 61.7715\n",
      "Epoch:  350 | train loss: 69.0631\n",
      "Epoch:  400 | train loss: 61.4743\n",
      "Epoch:  450 | train loss: 57.4262\n",
      "Epoch:  500 | train loss: 55.8625\n",
      "Epoch:  550 | train loss: 60.3286\n",
      "Epoch:  600 | train loss: 61.6888\n",
      "Epoch:  650 | train loss: 64.6009\n",
      "Epoch:  700 | train loss: 62.2901\n",
      "Epoch:  750 | train loss: 57.9014\n",
      "Epoch:  800 | train loss: 57.8888\n",
      "Epoch:  850 | train loss: 65.2580\n",
      "Epoch:  900 | train loss: 59.8597\n",
      "Epoch:  950 | train loss: 53.4957\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 82.4308\n",
      "Epoch:  50 | train loss: 90.2899\n",
      "Epoch:  100 | train loss: 84.0070\n",
      "Epoch:  150 | train loss: 92.3357\n",
      "Epoch:  200 | train loss: 89.7933\n",
      "Epoch:  250 | train loss: 95.8484\n",
      "Epoch:  300 | train loss: 91.0905\n",
      "Epoch:  350 | train loss: 90.5348\n",
      "Epoch:  400 | train loss: 83.4881\n",
      "Epoch:  450 | train loss: 87.3113\n",
      "Epoch:  500 | train loss: 85.1481\n",
      "Epoch:  550 | train loss: 80.6666\n",
      "Epoch:  600 | train loss: 91.9429\n",
      "Epoch:  650 | train loss: 97.1140\n",
      "Epoch:  700 | train loss: 81.9979\n",
      "Epoch:  750 | train loss: 93.8418\n",
      "Epoch:  800 | train loss: 93.8545\n",
      "Epoch:  850 | train loss: 84.6789\n",
      "Epoch:  900 | train loss: 93.6561\n",
      "Epoch:  950 | train loss: 77.8379\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 12.9343\n",
      "Epoch:  50 | train loss: 11.3244\n",
      "Epoch:  100 | train loss: 9.9887\n",
      "Epoch:  150 | train loss: 12.5835\n",
      "Epoch:  200 | train loss: 12.2301\n",
      "Epoch:  250 | train loss: 12.2535\n",
      "Epoch:  300 | train loss: 12.7793\n",
      "Epoch:  350 | train loss: 14.1817\n",
      "Epoch:  400 | train loss: 13.4243\n",
      "Epoch:  450 | train loss: 11.4079\n",
      "Epoch:  500 | train loss: 12.6586\n",
      "Epoch:  550 | train loss: 10.8385\n",
      "Epoch:  600 | train loss: 11.0573\n",
      "Epoch:  650 | train loss: 10.6619\n",
      "Epoch:  700 | train loss: 11.7543\n",
      "Epoch:  750 | train loss: 11.8575\n",
      "Epoch:  800 | train loss: 12.9940\n",
      "Epoch:  850 | train loss: 11.5818\n",
      "Epoch:  900 | train loss: 11.7046\n",
      "Epoch:  950 | train loss: 14.5953\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 24.3362\n",
      "Epoch:  50 | train loss: 22.9271\n",
      "Epoch:  100 | train loss: 22.0842\n",
      "Epoch:  150 | train loss: 22.7064\n",
      "Epoch:  200 | train loss: 23.3094\n",
      "Epoch:  250 | train loss: 24.4589\n",
      "Epoch:  300 | train loss: 21.2112\n",
      "Epoch:  350 | train loss: 21.2642\n",
      "Epoch:  400 | train loss: 22.4538\n",
      "Epoch:  450 | train loss: 22.0517\n",
      "Epoch:  500 | train loss: 23.4520\n",
      "Epoch:  550 | train loss: 23.6756\n",
      "Epoch:  600 | train loss: 23.7257\n",
      "Epoch:  650 | train loss: 23.2194\n",
      "Epoch:  700 | train loss: 20.5129\n",
      "Epoch:  750 | train loss: 20.0359\n",
      "Epoch:  800 | train loss: 25.5094\n",
      "Epoch:  850 | train loss: 18.8058\n",
      "Epoch:  900 | train loss: 23.3857\n",
      "Epoch:  950 | train loss: 23.9064\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 41.5247\n",
      "Epoch:  50 | train loss: 34.6702\n",
      "Epoch:  100 | train loss: 33.9729\n",
      "Epoch:  150 | train loss: 37.0707\n",
      "Epoch:  200 | train loss: 38.9401\n",
      "Epoch:  250 | train loss: 40.0779\n",
      "Epoch:  300 | train loss: 41.9502\n",
      "Epoch:  350 | train loss: 37.3721\n",
      "Epoch:  400 | train loss: 34.2431\n",
      "Epoch:  450 | train loss: 43.0733\n",
      "Epoch:  500 | train loss: 42.8775\n",
      "Epoch:  550 | train loss: 41.1822\n",
      "Epoch:  600 | train loss: 38.3993\n",
      "Epoch:  650 | train loss: 37.0538\n",
      "Epoch:  700 | train loss: 39.7512\n",
      "Epoch:  750 | train loss: 39.4278\n",
      "Epoch:  800 | train loss: 39.1573\n",
      "Epoch:  850 | train loss: 45.1246\n",
      "Epoch:  900 | train loss: 41.5375\n",
      "Epoch:  950 | train loss: 36.7584\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 59.8104\n",
      "Epoch:  50 | train loss: 66.5554\n",
      "Epoch:  100 | train loss: 64.2030\n",
      "Epoch:  150 | train loss: 59.1953\n",
      "Epoch:  200 | train loss: 59.2228\n",
      "Epoch:  250 | train loss: 53.7686\n",
      "Epoch:  300 | train loss: 62.4387\n",
      "Epoch:  350 | train loss: 61.2693\n",
      "Epoch:  400 | train loss: 55.8595\n",
      "Epoch:  450 | train loss: 53.9506\n",
      "Epoch:  500 | train loss: 59.5286\n",
      "Epoch:  550 | train loss: 56.6867\n",
      "Epoch:  600 | train loss: 61.2650\n",
      "Epoch:  650 | train loss: 53.5870\n",
      "Epoch:  700 | train loss: 61.5073\n",
      "Epoch:  750 | train loss: 61.3212\n",
      "Epoch:  800 | train loss: 59.0698\n",
      "Epoch:  850 | train loss: 70.2159\n",
      "Epoch:  900 | train loss: 68.0156\n",
      "Epoch:  950 | train loss: 58.8894\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 84.9104\n",
      "Epoch:  50 | train loss: 86.6624\n",
      "Epoch:  100 | train loss: 90.6734\n",
      "Epoch:  150 | train loss: 98.0999\n",
      "Epoch:  200 | train loss: 93.8256\n",
      "Epoch:  250 | train loss: 95.5036\n",
      "Epoch:  300 | train loss: 88.0838\n",
      "Epoch:  350 | train loss: 91.1129\n",
      "Epoch:  400 | train loss: 81.6786\n",
      "Epoch:  450 | train loss: 93.0105\n",
      "Epoch:  500 | train loss: 86.3138\n",
      "Epoch:  550 | train loss: 84.9416\n",
      "Epoch:  600 | train loss: 78.1560\n",
      "Epoch:  650 | train loss: 90.5821\n",
      "Epoch:  700 | train loss: 83.5528\n",
      "Epoch:  750 | train loss: 86.6349\n",
      "Epoch:  800 | train loss: 93.2060\n",
      "Epoch:  850 | train loss: 96.1936\n",
      "Epoch:  900 | train loss: 86.0857\n",
      "Epoch:  950 | train loss: 80.4064\n",
      "Training BJRNN\n",
      "Training dataset 0\n",
      "Epoch:  0 | train loss: 12.7467\n",
      "Epoch:  50 | train loss: 14.9040\n",
      "Epoch:  100 | train loss: 15.0756\n",
      "Epoch:  150 | train loss: 11.4361\n",
      "Epoch:  200 | train loss: 13.0681\n",
      "Epoch:  250 | train loss: 10.4964\n",
      "Epoch:  300 | train loss: 12.1572\n",
      "Epoch:  350 | train loss: 12.9016\n",
      "Epoch:  400 | train loss: 13.2388\n",
      "Epoch:  450 | train loss: 13.0337\n",
      "Epoch:  500 | train loss: 10.8253\n",
      "Epoch:  550 | train loss: 14.2989\n",
      "Epoch:  600 | train loss: 14.1569\n",
      "Epoch:  650 | train loss: 12.2651\n",
      "Epoch:  700 | train loss: 13.7050\n",
      "Epoch:  750 | train loss: 16.0626\n",
      "Epoch:  800 | train loss: 14.8561\n",
      "Epoch:  850 | train loss: 14.7301\n",
      "Epoch:  900 | train loss: 13.3532\n",
      "Epoch:  950 | train loss: 13.6944\n",
      "Training dataset 1\n",
      "Epoch:  0 | train loss: 21.2795\n",
      "Epoch:  50 | train loss: 24.4909\n",
      "Epoch:  100 | train loss: 23.0760\n",
      "Epoch:  150 | train loss: 24.6427\n",
      "Epoch:  200 | train loss: 20.0461\n",
      "Epoch:  250 | train loss: 21.4576\n",
      "Epoch:  300 | train loss: 23.2283\n",
      "Epoch:  350 | train loss: 22.2782\n",
      "Epoch:  400 | train loss: 23.6270\n",
      "Epoch:  450 | train loss: 21.6100\n",
      "Epoch:  500 | train loss: 21.8348\n",
      "Epoch:  550 | train loss: 23.7437\n",
      "Epoch:  600 | train loss: 25.3350\n",
      "Epoch:  650 | train loss: 23.2718\n",
      "Epoch:  700 | train loss: 23.7030\n",
      "Epoch:  750 | train loss: 24.2792\n",
      "Epoch:  800 | train loss: 24.1380\n",
      "Epoch:  850 | train loss: 21.6169\n",
      "Epoch:  900 | train loss: 24.1242\n",
      "Epoch:  950 | train loss: 22.6134\n",
      "Training dataset 2\n",
      "Epoch:  0 | train loss: 35.5281\n",
      "Epoch:  50 | train loss: 37.0521\n",
      "Epoch:  100 | train loss: 36.8948\n",
      "Epoch:  150 | train loss: 37.5407\n",
      "Epoch:  200 | train loss: 42.8445\n",
      "Epoch:  250 | train loss: 37.8198\n",
      "Epoch:  300 | train loss: 40.0167\n",
      "Epoch:  350 | train loss: 35.5914\n",
      "Epoch:  400 | train loss: 41.5580\n",
      "Epoch:  450 | train loss: 35.1760\n",
      "Epoch:  500 | train loss: 37.0929\n",
      "Epoch:  550 | train loss: 40.0555\n",
      "Epoch:  600 | train loss: 38.2040\n",
      "Epoch:  650 | train loss: 35.4358\n",
      "Epoch:  700 | train loss: 38.9003\n",
      "Epoch:  750 | train loss: 39.3232\n",
      "Epoch:  800 | train loss: 37.7646\n",
      "Epoch:  850 | train loss: 37.7056\n",
      "Epoch:  900 | train loss: 39.1582\n",
      "Epoch:  950 | train loss: 38.6448\n",
      "Training dataset 3\n",
      "Epoch:  0 | train loss: 60.1175\n",
      "Epoch:  50 | train loss: 61.2825\n",
      "Epoch:  100 | train loss: 67.2912\n",
      "Epoch:  150 | train loss: 66.3259\n",
      "Epoch:  200 | train loss: 57.1919\n",
      "Epoch:  250 | train loss: 62.4511\n",
      "Epoch:  300 | train loss: 56.0787\n",
      "Epoch:  350 | train loss: 64.1884\n",
      "Epoch:  400 | train loss: 53.0149\n",
      "Epoch:  450 | train loss: 63.7301\n",
      "Epoch:  500 | train loss: 62.3705\n",
      "Epoch:  550 | train loss: 60.0858\n",
      "Epoch:  600 | train loss: 60.6515\n",
      "Epoch:  650 | train loss: 61.5986\n",
      "Epoch:  700 | train loss: 60.1701\n",
      "Epoch:  750 | train loss: 56.6464\n",
      "Epoch:  800 | train loss: 58.3130\n",
      "Epoch:  850 | train loss: 58.9594\n",
      "Epoch:  900 | train loss: 49.7410\n",
      "Epoch:  950 | train loss: 59.6752\n",
      "Training dataset 4\n",
      "Epoch:  0 | train loss: 87.2609\n",
      "Epoch:  50 | train loss: 76.0790\n",
      "Epoch:  100 | train loss: 85.6996\n",
      "Epoch:  150 | train loss: 91.8841\n",
      "Epoch:  200 | train loss: 84.2103\n",
      "Epoch:  250 | train loss: 82.9248\n",
      "Epoch:  300 | train loss: 82.9118\n",
      "Epoch:  350 | train loss: 93.8712\n",
      "Epoch:  400 | train loss: 89.8760\n",
      "Epoch:  450 | train loss: 87.9923\n",
      "Epoch:  500 | train loss: 87.3987\n",
      "Epoch:  550 | train loss: 76.1499\n",
      "Epoch:  600 | train loss: 88.8353\n",
      "Epoch:  650 | train loss: 83.3579\n",
      "Epoch:  700 | train loss: 85.8890\n",
      "Epoch:  750 | train loss: 90.4910\n",
      "Epoch:  800 | train loss: 82.2149\n",
      "Epoch:  850 | train loss: 83.0901\n",
      "Epoch:  900 | train loss: 82.6814\n",
      "Epoch:  950 | train loss: 90.9011\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    for seed in range(5):\n",
    "        run_synthetic_experiments(experiment='time_dependent', \n",
    "                                  baseline=baseline,\n",
    "                                  n_train = 2000,\n",
    "                                  retrain_auxiliary=True,\n",
    "                                  save_model=True, \n",
    "                                  save_results=True,\n",
    "                                  rnn_mode='RNN',\n",
    "                                  seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint coverage**\n",
    "\n",
    "Prints mean joint coverage across the horizon (meanÂ±std of 5 random seeds, with each row indicating a different dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJRNN\n",
      "99.3 \\(\\pm\\) 0.5\\%\n",
      "96.8 \\(\\pm\\) 2.0\\%\n",
      "93.7 \\(\\pm\\) 1.9\\%\n",
      "90.0 \\(\\pm\\) 2.3\\%\n",
      "83.2 \\(\\pm\\) 0.6\\%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    print(baseline)\n",
    "    coverages_mean, coverages_std = get_joint_coverages(baseline, 'time_dependent', seeds=[0,1,2,3])\n",
    "    \n",
    "    for m, s in zip(coverages_mean, coverages_std):\n",
    "        print('{:.1f} \\\\(\\\\pm\\\\) {:.1f}\\\\%'.format(m, s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interval widths**\n",
    "\n",
    "Prints the baseline; meanÂ±std of interval width across horizons; rows denote the noise configuration and columns the random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BJRNN\n",
      "[27.90853504 27.51841832 32.80742456 35.54742906 39.38776248]\n",
      "[2.08241808 2.07214946 1.8318299  1.98028232 2.64290742]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for baseline in ['BJRNN']:\n",
    "    print(baseline)\n",
    "    widths_mean, widths_std = get_interval_widths(baseline, 'time_dependent', seeds=[0,1,3,4])\n",
    "    \n",
    "    print(widths_mean)\n",
    "    print(widths_std)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
