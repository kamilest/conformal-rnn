{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ancient-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.make_data import autoregressive\n",
    "# from models.conformal import nonconformity, cover, ConformalForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "former-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autoregressive_forecast_dataset(n_samples=100,\n",
    "                                             seq_len=100,\n",
    "                                             n_features=1,\n",
    "                                             X_mean=1,\n",
    "                                             X_variance=2,\n",
    "                                             noise_profile=None,\n",
    "                                             memory_factor=0.9,\n",
    "                                             mode=\"time-dependent\",\n",
    "                                             horizon=10):\n",
    "    total_seq_len = seq_len + horizon\n",
    "    # Create the input features of the generating process\n",
    "    X_gen = [np.random.normal(X_mean, X_variance, (total_seq_len,\n",
    "                                                   n_features))\n",
    "             for _ in range(n_samples)]\n",
    "    w = np.array([memory_factor ** k for k in range(total_seq_len)])\n",
    "\n",
    "    if noise_profile is None:\n",
    "        # default increasing noise profile\n",
    "        noise_profile = np.array(\n",
    "            [1 / (seq_len - 1) * k for k in range(total_seq_len)])\n",
    "\n",
    "    X = None  # X stores the time series values generated from features X_gen\n",
    "    if mode == \"noise-sweep\":\n",
    "        X = torch.FloatTensor(\n",
    "            [[(autoregressive(X_gen[k], w).reshape(total_seq_len, n_features) +\n",
    "               np.random.normal(0, noise_profile[u], (total_seq_len,\n",
    "                                                      n_features)))\n",
    "                  .reshape(total_seq_len, ) for k in range(n_samples)]\n",
    "             for u in range(len(noise_profile))])\n",
    "\n",
    "\n",
    "    elif mode == \"time-dependent\":\n",
    "        X = torch.FloatTensor(\n",
    "            [(autoregressive(X_gen[k], w)\n",
    "              .reshape(total_seq_len, n_features) + (\n",
    "                  torch.normal(mean=0.0, std=torch.tensor(noise_profile)))\n",
    "              .detach().numpy().reshape(-1, n_features)).reshape(\n",
    "                total_seq_len, )\n",
    "                for k in range(n_samples)])\n",
    "\n",
    "    # TODO clean up (un)squeezing.\n",
    "    Y = torch.FloatTensor(X[:, -horizon:])  # `horizon` of predictions\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X[:, :-horizon],\n",
    "                                        batch_first=True).unsqueeze(dim=-1)\n",
    "    \n",
    "    print(X.size(), Y.size())\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "western-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def nonconformity(output, target):\n",
    "    \"\"\"Measures the nonconformity between output and target time series.\"\"\"\n",
    "    # Average MAE loss for every step in the sequence.\n",
    "    return torch.nn.functional.l1_loss(output, target, reduction='none')\n",
    "\n",
    "\n",
    "def cover(pred, target):\n",
    "    # Returns True when the entire forecast fits into predicted conformal\n",
    "    # intervals.\n",
    "    # TODO joint vs independent coverage\n",
    "    return torch.all(\n",
    "        torch.logical_and(target >= pred[:, 0], target <= pred[:, 1])).item()\n",
    "\n",
    "\n",
    "class ConformalForecaster(nn.Module):\n",
    "    def __init__(self, embedding_size, input_size=1, output_size=1, horizon=1,\n",
    "                 error_rate=0.05):\n",
    "        super(ConformalForecaster, self).__init__()\n",
    "        # input_size indicates the number of features in the time series\n",
    "        # input_size=1 for univariate series.\n",
    "\n",
    "        # Encoder and forecaster can be the same (if embeddings are\n",
    "        # trained on `horizon`-step forecasts), but different models are\n",
    "        # possible.\n",
    "\n",
    "        # TODO try separate encoder and forecaster models.\n",
    "        # TODO try the RNN autoencoder trained on reconstruction error.\n",
    "        self.encoder = None\n",
    "\n",
    "        self.forecaster_rnn = nn.LSTM(input_size=input_size,\n",
    "                                      hidden_size=embedding_size,\n",
    "                                      batch_first=True)\n",
    "        self.forecaster_out = nn.Linear(embedding_size, output_size)\n",
    "\n",
    "        self.horizon = horizon\n",
    "        self.alpha = error_rate\n",
    "\n",
    "        self.num_train = None\n",
    "        self.calibration_scores = None\n",
    "        self.critical_calibration_scores = None\n",
    "\n",
    "    def forward(self, x, len_x):\n",
    "        # len_x : torch.LongTensor\n",
    "        # \t\t  Length of sequences (b, )\n",
    "        sorted_len, idx = len_x.sort(dim=0, descending=True)\n",
    "        sorted_x = x[idx]\n",
    "\n",
    "        # Convert to packed sequence batch\n",
    "        packed_x = torch.nn.utils.rnn.pack_padded_sequence(sorted_x,\n",
    "                                                           lengths=sorted_len,\n",
    "                                                           batch_first=True)\n",
    "\n",
    "        # [batch, seq_len, embedding_size]\n",
    "        packed_h, _ = self.forecaster_rnn(packed_x)\n",
    "\n",
    "        max_seq_len = x.size(1)\n",
    "        padded_output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_h,\n",
    "                                                                  batch_first=True,\n",
    "                                                                  total_length=max_seq_len)\n",
    "\n",
    "        _, reverse_idx = idx.sort(dim=0, descending=False)\n",
    "        padded_output = padded_output[reverse_idx]\n",
    "\n",
    "        # [batch, horizon, output_size, 1]\n",
    "        return self.forecaster_out(\n",
    "            padded_output[:, -self.horizon:, :]).unsqueeze(-1)\n",
    "\n",
    "    def fit(self, dataset, calibration_dataset, epochs, lr, batch_size=150):\n",
    "        # Train the forecaster to return correct multi-step predictions.\n",
    "        train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True)\n",
    "        self.num_train = len(dataset)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.\n",
    "\n",
    "            for sequences, targets in train_loader:  # iterate through batches\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = self(sequences)\n",
    "\n",
    "                loss = criterion(out, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            mean_train_loss = train_loss / len(train_loader)\n",
    "            if epoch % 50 == 0:\n",
    "                print(\n",
    "                    'Epoch: {}\\tTrain loss: {}'.format(epoch, mean_train_loss))\n",
    "\n",
    "        # Collect calibration scores\n",
    "        self.calibrate(calibration_dataset)\n",
    "\n",
    "    def calibrate(self, calibration_dataset):\n",
    "        \"\"\"\n",
    "        Computes the nonconformity scores for the calibration dataset.\n",
    "        \"\"\"\n",
    "        calibration_loader = torch.utils.data.DataLoader(calibration_dataset,\n",
    "                                                         batch_size=1)\n",
    "        calibration_scores = []\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.eval()\n",
    "            for sequences, targets in calibration_loader:\n",
    "                out = self(sequences)\n",
    "                calibration_scores.extend(\n",
    "                    nonconformity(out, targets).detach().numpy())\n",
    "\n",
    "        self.calibration_scores = torch.tensor(calibration_scores).T\n",
    "\n",
    "        # Given p_{z}:=\\frac{\\left|\\left\\{i=m+1, \\ldots, n+1: R_{i} \\geq R_{n+1}\\right\\}\\right|}{n-m+1}\n",
    "        # and the accepted R_{n+1} = \\Delta(y, f(x_{test})) are such that\n",
    "        # p_{z} > \\alpha we have that the nonconformity scores should be below\n",
    "        # the (corrected) (1 - alpha)% of calibration scores.\n",
    "\n",
    "        # TODO check: By applying (3) to Zcal, we get the sequence of\n",
    "        # non-conformity scores and then sort them in descending order\n",
    "        # α1, . . . , αq. Then, depending on the significance level ε, we define\n",
    "        # the index of the (1 − ε)-percentile non-conformity score, αs, such as\n",
    "        # s = ⌊ε(q + 1)⌋.\n",
    "        self.critical_calibration_scores = torch.tensor([np.quantile(\n",
    "            position_calibration_scores, q=1 - self.alpha * self.num_train / (\n",
    "                    self.num_train + 1))\n",
    "            for position_calibration_scores in self.calibration_scores])\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Forecasts the time series with conformal uncertainty intervals.\"\"\"\n",
    "        out = self(x).squeeze()\n",
    "        # TODO +/- nonconformity will not return *adaptive* interval widths.\n",
    "        # TODO correction for multiple comparisons for each multi-horizon step.\n",
    "        return torch.vstack([out - self.critical_calibration_scores,\n",
    "                             out + self.critical_calibration_scores]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "welcome-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100, 1]) torch.Size([1000, 10])\n",
      "torch.Size([1000, 100, 1]) torch.Size([1000, 10])\n",
      "torch.Size([100, 100, 1]) torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = generate_autoregressive_forecast_dataset(n_samples=1000, seq_len=100, horizon=10)\n",
    "calibration_dataset = generate_autoregressive_forecast_dataset(n_samples=1000, seq_len=100, horizon=10)\n",
    "test_dataset = generate_autoregressive_forecast_dataset(n_samples=100, seq_len=100, horizon=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: [n_samples, max_seq_len, n_features]\n",
    "# Y: [n_samples, horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nasty-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformalForecaster(embedding_size=8, horizon=10, error_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solar-stomach",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'len_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cd129ab61cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibration_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0d48fc1ae505>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, calibration_dataset, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'len_x'"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, calibration_dataset, epochs=10, lr=0.01, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "manufactured-reasoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.4111, 9.8009, 9.6471, 9.4183, 9.5524, 9.5123, 9.4077, 9.2933, 9.5192,\n",
       "        9.4380], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.critical_calibration_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "opening-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved coverage: 0.77\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "c = []\n",
    "for sequence, target in test_dataset:\n",
    "    sequence = sequence.unsqueeze(dim=0)\n",
    "    pred = model.predict(sequence)\n",
    "    c.append(cover(pred, target))\n",
    "print('Achieved coverage: {}'.format(np.mean(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "later-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cover(pred, target):\n",
    "    # Returns True when the entire forecast fits into predicted conformal\n",
    "    # intervals.\n",
    "    return torch.all(\n",
    "        torch.logical_and(target >= pred[:, 0], target <= pred[:, 1])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "generic-retailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 8])\n",
      "torch.Size([1, 10, 1, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for sequence, target in test_dataset:\n",
    "    sequence = sequence.unsqueeze(dim=0)\n",
    "    h, _ = model.forecaster_rnn(sequence)\n",
    "    print(h.size())\n",
    "    # [batch, horizon, output_size, 1]\n",
    "    out = model.forecaster_out(h[:, -10:, :]).unsqueeze(-1)\n",
    "    print(out.size())\n",
    "    print(model.critical_calibration_scores.size())\n",
    "    print(out.squeeze().size())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "phantom-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "num_train = 1000\n",
    "\n",
    "def calibrate(calibration_dataset):\n",
    "    \"\"\"\n",
    "    Computes the nonconformity scores for the calibration dataset.\n",
    "    \"\"\"\n",
    "    calibration_loader = torch.utils.data.DataLoader(calibration_dataset,\n",
    "                                                     batch_size=1)\n",
    "    calibration_scores = []\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        model.eval()\n",
    "        for sequences, targets in calibration_loader:\n",
    "            out = model(sequences)\n",
    "            calibration_scores.extend(nonconformity(out, targets).detach().numpy())\n",
    "\n",
    "    calibration_scores = torch.tensor(calibration_scores).T\n",
    "\n",
    "    # Given p_{z}:=\\frac{\\left|\\left\\{i=m+1, \\ldots, n+1: R_{i} \\geq R_{n+1}\\right\\}\\right|}{n-m+1}\n",
    "    # and the accepted R_{n+1} = \\Delta(y, f(x_{test})) are such that\n",
    "    # p_{z} > \\alpha we have that the nonconformity scores should be below\n",
    "    # the (corrected) alpha% of calibration scores.\n",
    "    critical_calibration_scores = torch.tensor([np.quantile(\n",
    "        position_calibration_scores, q= alpha * num_train / (num_train + 1))\n",
    "        for position_calibration_scores in calibration_scores])\n",
    "    print(critical_calibration_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "scenic-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1335, 0.2231, 0.2458, 0.2786, 0.2465, 0.2878, 0.3393, 0.3049, 0.2977,\n",
      "        0.3465], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "calibrate(calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "figured-swift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500499500499501"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - alpha * num_train / (num_train + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "conceptual-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def autoregressive(X_gen, w):\n",
    "    \"\"\" Generates a single time series example. \"\"\"\n",
    "    return np.array(\n",
    "        [np.sum(X_gen[0:k + 1] * np.flip(w[0:k + 1]).reshape(-1, 1)) for k in\n",
    "         range(len(X_gen))])\n",
    "\n",
    "\n",
    "seq_len = 10\n",
    "horizon = 10\n",
    "n_samples = 100\n",
    "X_mean = 1\n",
    "X_variance = 2\n",
    "n_features = 1\n",
    "memory_factor = 0.9\n",
    "\n",
    "# sequence_lengths = [seq_len + horizon] * n_samples\n",
    "sequence_lengths = range(horizon + 1, n_samples + horizon + 1)\n",
    "max_seq_len = np.max(sequence_lengths)\n",
    "\n",
    "# Create the input features of the generating process\n",
    "X_gen = [np.random.normal(X_mean, X_variance, (seq_len,\n",
    "                                               n_features))\n",
    "         for seq_len in sequence_lengths]\n",
    "\n",
    "# TODO determine how do weights and noise profile change depending on the\n",
    "# length of series.\n",
    "w = np.array([memory_factor ** k for k in range(max_seq_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "limiting-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_profiles = [[1 / (seq_len - 1) * k for k in range(max(1, seq_len))] for seq_len in sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "regular-relay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gen[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "continent-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = [np.random.normal(0., noise_profile).reshape(-1, n_features) for noise_profile in noise_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "trained-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = [autoregressive(X_gen[k], w).reshape(sequence_lengths[k], n_features) for k in range(n_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "noted-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = [torch.tensor(i + j) for i, j in zip(ar, noise)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "international-bruce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "defensive-sydney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "adjustable-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[autoregressive(X_gen[k], w).reshape(-1, n_features) for k in range(n_samples)][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "certain-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "opened-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for seq in X_full:\n",
    "    seq_len = len(seq)\n",
    "    if seq_len >= 2 * horizon:\n",
    "        X.append(seq[:-horizon])\n",
    "        Y.append(seq[-horizon:])\n",
    "    elif seq_len > horizon:\n",
    "        X.append(seq[:seq_len - horizon])\n",
    "        Y.append(seq[-(seq_len - horizon):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "gothic-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# for x in X:\n",
    "print(len(X[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "conditional-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_full: 11\tX: 1\tY: 1\n",
      "X_full: 12\tX: 2\tY: 2\n",
      "X_full: 13\tX: 3\tY: 3\n",
      "X_full: 14\tX: 4\tY: 4\n",
      "X_full: 15\tX: 5\tY: 5\n",
      "X_full: 16\tX: 6\tY: 6\n",
      "X_full: 17\tX: 7\tY: 7\n",
      "X_full: 18\tX: 8\tY: 8\n",
      "X_full: 19\tX: 9\tY: 9\n",
      "X_full: 20\tX: 10\tY: 10\n",
      "X_full: 21\tX: 11\tY: 10\n",
      "X_full: 22\tX: 12\tY: 10\n",
      "X_full: 23\tX: 13\tY: 10\n",
      "X_full: 24\tX: 14\tY: 10\n",
      "X_full: 25\tX: 15\tY: 10\n",
      "X_full: 26\tX: 16\tY: 10\n",
      "X_full: 27\tX: 17\tY: 10\n",
      "X_full: 28\tX: 18\tY: 10\n",
      "X_full: 29\tX: 19\tY: 10\n",
      "X_full: 30\tX: 20\tY: 10\n",
      "X_full: 31\tX: 21\tY: 10\n",
      "X_full: 32\tX: 22\tY: 10\n",
      "X_full: 33\tX: 23\tY: 10\n",
      "X_full: 34\tX: 24\tY: 10\n",
      "X_full: 35\tX: 25\tY: 10\n",
      "X_full: 36\tX: 26\tY: 10\n",
      "X_full: 37\tX: 27\tY: 10\n",
      "X_full: 38\tX: 28\tY: 10\n",
      "X_full: 39\tX: 29\tY: 10\n",
      "X_full: 40\tX: 30\tY: 10\n",
      "X_full: 41\tX: 31\tY: 10\n",
      "X_full: 42\tX: 32\tY: 10\n",
      "X_full: 43\tX: 33\tY: 10\n",
      "X_full: 44\tX: 34\tY: 10\n",
      "X_full: 45\tX: 35\tY: 10\n",
      "X_full: 46\tX: 36\tY: 10\n",
      "X_full: 47\tX: 37\tY: 10\n",
      "X_full: 48\tX: 38\tY: 10\n",
      "X_full: 49\tX: 39\tY: 10\n",
      "X_full: 50\tX: 40\tY: 10\n",
      "X_full: 51\tX: 41\tY: 10\n",
      "X_full: 52\tX: 42\tY: 10\n",
      "X_full: 53\tX: 43\tY: 10\n",
      "X_full: 54\tX: 44\tY: 10\n",
      "X_full: 55\tX: 45\tY: 10\n",
      "X_full: 56\tX: 46\tY: 10\n",
      "X_full: 57\tX: 47\tY: 10\n",
      "X_full: 58\tX: 48\tY: 10\n",
      "X_full: 59\tX: 49\tY: 10\n",
      "X_full: 60\tX: 50\tY: 10\n",
      "X_full: 61\tX: 51\tY: 10\n",
      "X_full: 62\tX: 52\tY: 10\n",
      "X_full: 63\tX: 53\tY: 10\n",
      "X_full: 64\tX: 54\tY: 10\n",
      "X_full: 65\tX: 55\tY: 10\n",
      "X_full: 66\tX: 56\tY: 10\n",
      "X_full: 67\tX: 57\tY: 10\n",
      "X_full: 68\tX: 58\tY: 10\n",
      "X_full: 69\tX: 59\tY: 10\n",
      "X_full: 70\tX: 60\tY: 10\n",
      "X_full: 71\tX: 61\tY: 10\n",
      "X_full: 72\tX: 62\tY: 10\n",
      "X_full: 73\tX: 63\tY: 10\n",
      "X_full: 74\tX: 64\tY: 10\n",
      "X_full: 75\tX: 65\tY: 10\n",
      "X_full: 76\tX: 66\tY: 10\n",
      "X_full: 77\tX: 67\tY: 10\n",
      "X_full: 78\tX: 68\tY: 10\n",
      "X_full: 79\tX: 69\tY: 10\n",
      "X_full: 80\tX: 70\tY: 10\n",
      "X_full: 81\tX: 71\tY: 10\n",
      "X_full: 82\tX: 72\tY: 10\n",
      "X_full: 83\tX: 73\tY: 10\n",
      "X_full: 84\tX: 74\tY: 10\n",
      "X_full: 85\tX: 75\tY: 10\n",
      "X_full: 86\tX: 76\tY: 10\n",
      "X_full: 87\tX: 77\tY: 10\n",
      "X_full: 88\tX: 78\tY: 10\n",
      "X_full: 89\tX: 79\tY: 10\n",
      "X_full: 90\tX: 80\tY: 10\n",
      "X_full: 91\tX: 81\tY: 10\n",
      "X_full: 92\tX: 82\tY: 10\n",
      "X_full: 93\tX: 83\tY: 10\n",
      "X_full: 94\tX: 84\tY: 10\n",
      "X_full: 95\tX: 85\tY: 10\n",
      "X_full: 96\tX: 86\tY: 10\n",
      "X_full: 97\tX: 87\tY: 10\n",
      "X_full: 98\tX: 88\tY: 10\n",
      "X_full: 99\tX: 89\tY: 10\n",
      "X_full: 100\tX: 90\tY: 10\n",
      "X_full: 101\tX: 91\tY: 10\n",
      "X_full: 102\tX: 92\tY: 10\n",
      "X_full: 103\tX: 93\tY: 10\n",
      "X_full: 104\tX: 94\tY: 10\n",
      "X_full: 105\tX: 95\tY: 10\n",
      "X_full: 106\tX: 96\tY: 10\n",
      "X_full: 107\tX: 97\tY: 10\n",
      "X_full: 108\tX: 98\tY: 10\n",
      "X_full: 109\tX: 99\tY: 10\n",
      "X_full: 110\tX: 100\tY: 10\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print('X_full: {}\\tX: {}\\tY: {}'.format(len(X_full[i]), len(X[i]), len(Y[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "unable-metadata",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "vanilla-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "headed-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.nn.utils.rnn.pad_sequence(X, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "scientific-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "white-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tensor = torch.nn.utils.rnn.pad_sequence(Y, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "minute-branch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: [n_samples, max_seq_len, n_features]\n",
    "# Y: [n_samples, horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "collectible-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(sequence_lengths).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "needed-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7fa6b6de9c10>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils.data.TensorDataset(X_tensor, Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-employee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
