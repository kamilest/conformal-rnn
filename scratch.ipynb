{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lovely-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.make_data import autoregressive, generate_autoregressive_forecast_dataset\n",
    "from models.conformal import nonconformity, cover, ConformalForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separate-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_autoregressive_forecast_dataset(n_samples=1000, seq_len=100, horizon=10)\n",
    "calibration_dataset = generate_autoregressive_forecast_dataset(n_samples=1000, seq_len=100, horizon=10)\n",
    "test_dataset = generate_autoregressive_forecast_dataset(n_samples=100, seq_len=100, horizon=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "french-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autoregressive_forecast_dataset(n_samples=100,\n",
    "                                             seq_len=100,\n",
    "                                             n_features=1,\n",
    "                                             X_mean=1,\n",
    "                                             X_variance=2,\n",
    "                                             noise_profile=None,\n",
    "                                             memory_factor=0.9,\n",
    "                                             mode=\"time-dependent\",\n",
    "                                             horizon=10):\n",
    "    total_seq_len = seq_len + horizon\n",
    "    # Create the input features of the generating process\n",
    "    X_gen = [np.random.normal(X_mean, X_variance, (total_seq_len,\n",
    "                                                   n_features))\n",
    "             for _ in range(n_samples)]\n",
    "    w = np.array([memory_factor ** k for k in range(total_seq_len)])\n",
    "\n",
    "    if noise_profile is None:\n",
    "        # default increasing noise profile\n",
    "        noise_profile = np.array(\n",
    "            [1 / (seq_len - 1) * k for k in range(total_seq_len)])\n",
    "\n",
    "    X = None  # X stores the time series values generated from features X_gen\n",
    "    if mode == \"noise-sweep\":\n",
    "        X = torch.FloatTensor(\n",
    "            [[(autoregressive(X_gen[k], w).reshape(total_seq_len, n_features) +\n",
    "               np.random.normal(0, noise_profile[u], (total_seq_len,\n",
    "                                                      n_features)))\n",
    "                  .reshape(total_seq_len, ) for k in range(n_samples)]\n",
    "             for u in range(len(noise_profile))])\n",
    "\n",
    "\n",
    "    elif mode == \"time-dependent\":\n",
    "        X = torch.FloatTensor([(autoregressive(X_gen[k], w)\n",
    "              .reshape(total_seq_len, n_features) + (\n",
    "                  torch.normal(mean=0.0, std=torch.tensor(noise_profile)))\n",
    "              .detach().numpy().reshape(-1, n_features)).reshape(\n",
    "                total_seq_len, )\n",
    "                for k in range(n_samples)])\n",
    "        \n",
    "\n",
    "    # TODO clean up (un)squeezing.\n",
    "    Y = torch.FloatTensor(X[:, -horizon:])  # `horizon` of predictions\n",
    "    X = torch.nn.utils.rnn.pad_sequence(X[:, :-horizon], batch_first=True).unsqueeze(dim=-1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "manual-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def nonconformity(output, target):\n",
    "    \"\"\"Measures the nonconformity between output and target time series.\"\"\"\n",
    "    # Average MAE loss for every step in the sequence.\n",
    "    return torch.nn.functional.l1_loss(output, target, reduction='none')\n",
    "\n",
    "\n",
    "class ConformalForecaster(nn.Module):\n",
    "    def __init__(self, embedding_size, input_size=1, horizon=1, alpha=0.05):\n",
    "        super(ConformalForecaster, self).__init__()\n",
    "        # input_size indicates the number of features in the time series\n",
    "        # input_size=1 for univariate series.\n",
    "\n",
    "        # Encoder and forecaster can be the same (if embeddings are\n",
    "        # trained on `horizon`-step forecasts), but different models are\n",
    "        # possible.\n",
    "\n",
    "        # TODO try separate encoder and forecaster models.\n",
    "        # TODO try the RNN autoencoder trained on reconstruction error.\n",
    "        self.encoder = None\n",
    "\n",
    "        # Single-shot multi-output univariate time series forecaster.\n",
    "        # https://www.tensorflow.org/tutorials/structured_data/time_series#rnn_2\n",
    "        # TODO consider autoregressive multi-output model:\n",
    "        # https://www.tensorflow.org/tutorials/structured_data/time_series#advanced_autoregressive_model\n",
    "        self.forecaster_rnn = nn.LSTM(input_size=input_size,\n",
    "                                      hidden_size=embedding_size,\n",
    "                                      batch_first=True)\n",
    "        self.forecaster_out = nn.Linear(embedding_size, horizon)\n",
    "\n",
    "        self.num_train = None\n",
    "        self.calibration_scores = None\n",
    "        self.critical_calibration_score = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, c_n) = self.forecaster_rnn(x)\n",
    "        out = self.forecaster_out(h_n)\n",
    "\n",
    "        return out.squeeze(dim=0)\n",
    "\n",
    "    def fit(self, dataset, calibration_dataset, epochs, lr, batch_size=150):\n",
    "        # Train the forecaster to return correct multi-step predictions.\n",
    "        train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True)\n",
    "        self.num_train = len(dataset)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.\n",
    "\n",
    "            for sequences, targets in train_loader:  # iterate through batches\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = self(sequences)\n",
    "\n",
    "                loss = criterion(out, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            mean_train_loss = train_loss / len(train_loader)\n",
    "            if epoch % 50 == 0:\n",
    "                print('Epoch: {}\\tTrain loss: {}'.format(epoch, mean_train_loss))\n",
    "\n",
    "        # Collect calibration scores\n",
    "        self.calibrate(calibration_dataset)\n",
    "\n",
    "    def calibrate(self, calibration_dataset):\n",
    "        \"\"\"\n",
    "        Computes the nonconformity scores for the calibration dataset.\n",
    "        \"\"\"\n",
    "        calibration_loader = torch.utils.data.DataLoader(calibration_dataset,\n",
    "                                                         batch_size=1)\n",
    "        calibration_scores = []\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            self.eval()\n",
    "            for sequences, targets in calibration_loader:\n",
    "                out = self(sequences)\n",
    "                calibration_scores.extend(nonconformity(out, targets).detach().numpy())\n",
    "\n",
    "        self.calibration_scores = torch.tensor(calibration_scores).T\n",
    "\n",
    "        # Given p_{z}:=\\frac{\\left|\\left\\{i=m+1, \\ldots, n+1: R_{i} \\geq R_{n+1}\\right\\}\\right|}{n-m+1}\n",
    "        # and the accepted R_{n+1} = \\Delta(y, f(x_{test})) are such that\n",
    "        # p_{z} > \\alpha we have that the nonconformity scores should be below\n",
    "        # the (corrected) alpha% of calibration scores.\n",
    "        self.critical_calibration_scores = torch.tensor([np.quantile(\n",
    "            position_calibration_scores, q=1 - self.alpha * self.num_train / (self.num_train + 1))\n",
    "            for position_calibration_scores in self.calibration_scores])\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Forecasts the time series with conformal uncertainty intervals.\"\"\"\n",
    "        out = self(x)\n",
    "        # TODO +/- nonconformity will not return *adaptive* interval widths.\n",
    "        return torch.vstack([out - self.critical_calibration_scores,\n",
    "                             out + self.critical_calibration_scores]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "departmental-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformalForecaster(embedding_size=8, horizon=10, error_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "homeless-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 117.00947952270508\n",
      "Epoch: 50\tTrain loss: 15.443363380432128\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, calibration_dataset, epochs=100, lr=0.01, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "selective-throat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved coverage: 0.72\n"
     ]
    }
   ],
   "source": [
    "test_dataset = generate_autoregressive_forecast_dataset(n_samples=100, seq_len=100, horizon=10)\n",
    "model.eval()\n",
    "c = []\n",
    "for sequences, target in test_dataset:\n",
    "    sequences = sequences.unsqueeze(dim=0)\n",
    "    out = model(sequences).squeeze()\n",
    "    pred = torch.vstack([out - model.critical_calibration_scores,\n",
    "                         out + model.critical_calibration_scores]).T\n",
    "    c.append(cover(pred, target))\n",
    "print('Achieved coverage: {}'.format(np.mean(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "young-imaging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fitting-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "num_train = 1000\n",
    "\n",
    "def calibrate(calibration_dataset):\n",
    "    \"\"\"\n",
    "    Computes the nonconformity scores for the calibration dataset.\n",
    "    \"\"\"\n",
    "    calibration_loader = torch.utils.data.DataLoader(calibration_dataset,\n",
    "                                                     batch_size=1)\n",
    "    calibration_scores = []\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        model.eval()\n",
    "        for sequences, targets in calibration_loader:\n",
    "            out = model(sequences)\n",
    "            calibration_scores.extend(nonconformity(out, targets).detach().numpy())\n",
    "\n",
    "    calibration_scores = torch.tensor(calibration_scores).T\n",
    "\n",
    "    # Given p_{z}:=\\frac{\\left|\\left\\{i=m+1, \\ldots, n+1: R_{i} \\geq R_{n+1}\\right\\}\\right|}{n-m+1}\n",
    "    # and the accepted R_{n+1} = \\Delta(y, f(x_{test})) are such that\n",
    "    # p_{z} > \\alpha we have that the nonconformity scores should be below\n",
    "    # the (corrected) alpha% of calibration scores.\n",
    "    critical_calibration_scores = torch.tensor([np.quantile(\n",
    "        position_calibration_scores, q= alpha * num_train / (num_train + 1))\n",
    "        for position_calibration_scores in calibration_scores])\n",
    "    print(critical_calibration_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "successful-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1335, 0.2231, 0.2458, 0.2786, 0.2465, 0.2878, 0.3393, 0.3049, 0.2977,\n",
      "        0.3465], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "calibrate(calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "concerned-brush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500499500499501"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - alpha * num_train / (num_train + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-gasoline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
